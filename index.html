<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>Behrooz File System (BFS) by bshafiee</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>Behrooz File System (BFS)</h1>
        <h2>An in-memory distributed POSIX-like file system</h2>
        <a href="https://github.com/bshafiee/BFS" class="button"><small>View project on</small> GitHub</a>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <h3>
<a id="welcome-to-bfs" class="anchor" href="#welcome-to-bfs" aria-hidden="true"><span class="octicon octicon-link"></span></a>Welcome to BFS</h3>

<p>Behrooz File System (BFS) is presented, which provides an in-memory distributed file system. BFS is a simple design which combines the best of in- memory and remote file systems. BFS stores data in the main memory of commodity servers and provides a shared unified file system view over them. BFS utilizes backend storage to provide persistency and availability. Unlike most existing distributed in-memory storage systems, BFS supports a general purpose POSIX-like file interface. BFS is built by grouping multiple servers’ memory together; therefore, if applications and BFS servers are co-located, BFS is a highly efficient design because this architecture minimizes inter-node communication. BFS nodes can communicate thorough a TCP connection or a faster ZERO_Networking solution. In ZERO_Networking mode the regular operating system network stack is bypassed and raw packets are shared between userspace and kernel.</p>

<p>BFS has been developed as part of Behrooz Shafiee Sarjaz master thesis under supervision of professor <a href="https://cs.uwaterloo.ca/%7Emkarsten/">Martin Karsten</a> at <a href="https://cs.uwaterloo.ca/">Cheriton School of Computer Science</a>, <a href="https://uwaterloo.ca/">University of Waterloo</a>.</p>

<h3>
<a id="usage" class="anchor" href="#usage" aria-hidden="true"><span class="octicon octicon-link"></span></a>Usage</h3>

<p>Please refer to the <a href="https://github.com/bshafiee/BFS/wiki/Behrooz-File-System-%28BFS%29--Wiki!">Wiki page</a> for more information.</p>

<h3>
<a id="preliminary-evaluation" class="anchor" href="#preliminary-evaluation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Preliminary evaluation</h3>

<p>BFS can be used as a standalone filesystem on a single machine (STANDALONE_MODE) or even as a replacement for Apache Hadoop HDFS. In order to evaluate BFS performance, three different sets of experiments were defined. First, BFS was evaluated using IOZONE filesystem benchmark tool in standalone mode in which all IO operations are performed on a single machine. Second, BFS remote operations were evaluated using IOZONE; however, this time all IO operation were performed on a remote node. Finally, BFS was used as the underlying Hadoop mapreduce distributed file system.</p>

<p><strong>Evaluation Environment</strong></p>

<p>All the evaluations of BFS are run in a cluster of 16 machines at the University of Waterloo. The cluster contains one head node and 15 compute nodes.</p>

<p><strong>1x Head node: Supermicro SSG-6047R-E1R36L</strong></p>

<ul>
<li>2x Intel E5-2630v2 CPU</li>
<li>256 GB RAM</li>
<li>14x 2TB 7200RPM SAS2 hard drives (LSI HBA-connected) ◦ 1x Intel S3700 400 GB SATA3 SSD</li>
<li>4x Intel i350 gigabit Ethernet ports</li>
<li>1x Mellanox 40 GbE QSFP port</li>
</ul>

<p><strong>15x Computer nodes: Supermicro SYS-6017R-TDF</strong></p>

<ul>
<li>2x Intel E5-2620v2 CPU</li>
<li>64 GB RAM</li>
<li>3x 1TB SATA3 hard drives</li>
<li>1x Intel S3700 200 GB SATA3 SSD ◦ 2x Intel i350 gigabit Ethernet ports ◦ 1x Mellanox 10 GbE SFP port</li>
<li>All nodes run Debian GNU/Linux 8.0 (jessie) with Linux kernel 3.16.0-4-amd64</li>
</ul>

<p><strong>Standalone mode</strong></p>

<p>In this mode BFS was evaluated by IOZONE for write, rewrite, read, reread, random read, and random write operations, all performed locally on a single node. Block size was varied from standard 4 KB to 16 MB with file size from 1MB to 1GB. Unfortunately, fuse library which is used by BFS to provide a transparent POSIX compatible interface to applications does not support block sizes larger than 128KB. Therefore, it was patched to support block sizes up to 16 MB. </p>

<p><strong>Read on a single node locally:</strong></p>

<p><img src="http://i.imgur.com/TWq1BLL.png" alt="Read on a single node locally."></p>

<p><strong>Write on a single node locally:</strong></p>

<p><img src="http://i.imgur.com/iVNg9e6.png" alt="Write on a single node locally."></p>

<p>As it can be seen from the graphs increasing block size improves IO rate; however, the peak rate happens at 64KB and 128KB block size and does not increase anymore.</p>

<p><strong>Network Mode</strong></p>

<p>In this mode, one of the cluster nodes was configured to zero byte available memory and the rest of cluster nodes were configured with 70% of physical memory (1.3GB) as available memory. Then the node with zero byte available memory was instructed to run IOZONE benchmark; therefore, all of it’s operations were performed on remote nodes. In fact, even though the whole cluster was available only one other node would be used in this scenario because IOZONE performs all of its operations on one file;therefore, whoever is storing that file will perform IO operations instructed remotely by the zero byte node. </p>

<p>BFS implements two modes of remote communications: TCP and ZERO networking. TCP mode was implemented by a non­blocking socket model using select IO model with Poco C++ library. ZERO mode was implemented using PF_RING library. PF_RING is a high speed packet capture library which is usually used by packet filtering applications. The main benefit of PF_RING library is capturing and sending packets without going through kernel standard data structures. BFS utilizes PF_RING to communicate among nodes in the same subnet. Due to the fact that in a subnet with Ethernet Flow-­Control enabled on all nodes and switches no packet is lost, BFS has no need for a reliable transport protocol. Thus, BFS implements only a minimal transport protocol capable of multiplexing and demultiplexing different tasks simultaneously (Similar to the concept of a socket).</p>

<p><strong>Read remotely TCP:</strong></p>

<p><img src="http://i.imgur.com/MNYOBbU.png" alt="Read Remotely TCP"></p>

<p><strong>Read Remotely ZERO_Networking:</strong></p>

<p><img src="http://i.imgur.com/9Uwootz.png" alt="Read Remote Zero_Networking"></p>

<p><strong>Write remotely TCP:</strong></p>

<p><img src="http://i.imgur.com/kTxa76a.png" alt="Write Remote TCP"></p>

<p><strong>Write Remotely ZERO_Networking:</strong></p>

<p><img src="http://i.imgur.com/GdowVeQ.png" alt="Read Remote Zero_Networking"></p>

<p><strong>Hadoop Mapreduce DFSIO Benchmark</strong></p>

<p>Hadoop mapreduce framework uses Hadoop Distributed File System(HDFS) as the default file system. HDFS is a highly distributed, replicated, scalable and portable file system written in JAVA. HDFS is accessed in mapreduce  applications using a standard set of libraries and it does not provide a POSIX interface;therefore, in order to use HDFS, developers need to first copy input data into hdfs and then after processing is finished copy back results from HDFS to the local filesystem. Moreover, HDFS does not support random reads and writes. Therefore,<br>
using BFS as HADOOP filesystem provides many opportunities to improve the performance of mapreduce applications.  </p>

<p>In order to solve the problem of copying data back and forth, some parts of Hadoop mapreduce framework need to be modified. However, the goal was not to modify the code and only Hadoop configuration files were hacked to use BFS instead of HDFS.</p>

<p>After replacing HDFS with BFS, Hadoop DFSIO test was used to measure the improvements made by using BFS instead of HDFS. DFSIO test is a stress test originally developed by HADOOP team to stress HDFS and find IO bottlenecks. DFSIO performs distributed writes and reads and reports the average IO rate. Benchmark inputs include number of files, size of each file, and block size.</p>

<p>DFSIO test was performed 10 times using HDFS, BFS_TCP, and BFS_ZERO mode. The file size was set to 1 GB and number of files was set to 4.  </p>

<p>** Write Test **</p>

<p><img src="http://i.imgur.com/5T5YZPt.png" alt="Write DFSIO"></p>

<p>it can be seen from the graph that both BFS TCP and BFS ZERO perform significantly better than HDFS (10X faster) with a small error bar (std). This shows that all the engaging node performed write locally (in their memory). </p>

<p>** Read test **</p>

<p><img src="http://i.imgur.com/amD9U5F.png" alt="Read DFSIO"></p>

<p>There is a wide range of Avg IO rate in the read test results from 20MB to 140MB per second. This is due to the fact that some of the engaged nodes read local data (in their memory) and some of them read data in the remote nodes. This can be proved by noticing the error bar on each of data points. For example when in BFS_ZERO the avg rate is 135MB/Sec the std is almost zero meaning that all the nodes read their data locally. Similarly, when the average rate is so small 25MB/Sec the std is also significantly small meaning that all the nodes performed their read remotely. Finally, in between average IO rates such as 50MB/Sec have the highest std because some of the nodes performed their reads locally and some remotely. </p>

<p>Another noticeable point about read was that even though the block size was set to 16MB, non of reads were performed in 16MB and the all used a very small varying block size from 4KB to around 512KB. This can explain the low IO rate when nodes read data remotely.</p>

<h3>
<a id="disclaimer" class="anchor" href="#disclaimer" aria-hidden="true"><span class="octicon octicon-link"></span></a>DISCLAIMER</h3>

<p>BFS is still under heavy development and is not recommended for production level use. You can use BFS at your own risk.</p>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Authors and Contributors</h3>

<p>BFS has been developed as part of Behrooz Shafiee Sarjaz (<a href="https://github.com/bshafiee" class="user-mention">@bshafiee</a>) master thesis under supervision of professor <a href="https://cs.uwaterloo.ca/%7Emkarsten/">Martin Karsten</a> at <a href="https://cs.uwaterloo.ca/">Cheriton School of Computer Science</a>, <a href="https://uwaterloo.ca/">University of Waterloo</a>.</p>

<h3>
<a id="contact" class="anchor" href="#contact" aria-hidden="true"><span class="octicon octicon-link"></span></a>Contact</h3>

<p>Reach me at <a href="mailto:bshafiee@uwaterloo.ca">bshafiee@uwaterloo.ca</a> </p>
        </section>

        <aside id="sidebar">
          <a href="https://github.com/bshafiee/BFS/zipball/master" class="button">
            <small>Download</small>
            .zip file
          </a>
          <a href="https://github.com/bshafiee/BFS/tarball/master" class="button">
            <small>Download</small>
            .tar.gz file
          </a>

          <p class="repo-owner"><a href="https://github.com/bshafiee/BFS"></a> is maintained by <a href="https://github.com/bshafiee">bshafiee</a>.</p>

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p>
        </aside>
      </div>
    </div>

  
  </body>
</html>
